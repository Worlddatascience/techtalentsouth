{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Master Big Data In Little Time\n",
    "## [Table Of Contents](../index.ipynb)\n",
    "### History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big Data** is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.\n",
    "\n",
    "**Big Data** challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy and data source. Big data was originally associated with three key concepts: ***volume, variety, and velocity***. When we handle big data, we may not sample but simply observe and track what happens. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/3v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data History Important Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/infogrowth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bubonic Plague\n",
    "\n",
    "In 1663, John Graunt dealt with “overwhelming amounts of information” as well, while he studied the **bubonic plague**, which was currently ravaging Europe. Graunt used statistics and is credited with being the first person to use statistical data analysis. In the early 1800s, the field of statistics expanded to include collecting and analyzing data.\n",
    "\n",
    "### 1880s Census Bureau\n",
    "\n",
    "Data became a problem for the **U.S. Census Bureau** in 1880. They estimated it would take eight years to handle and process the data collected during the 1880 census, and predicted the data from the 1890 census would take more than 10 years to process. Fortunately, in 1881, a young man working for the bureau, named Herman Hollerith, created the Hollerith Tabulating Machine. His invention was based on the punch cards designed for controlling the patterns woven by mechanical looms. His tabulating machine reduced ten years of labor into three months of labor.\n",
    "\n",
    "In 1927, Fritz Pfleumer, an Austrian-German engineer, developed a means of **storing information magnetically on tape**. Pfleumer had devised a method for adhering metal stripes to cigarette papers (to keep a smokers’ lips from being stained by the rolling papers available at the time), and decided he could use this technique to create a magnetic strip, which could then be used to replace wire recording technology. After experiments with a variety of materials, he settled on a very thin paper, striped with iron oxide powder and coated with lacquer, for his patent in 1928.\n",
    "\n",
    "### World War II\n",
    "\n",
    "During World War II (more specifically 1943), the British, desperate to crack **Nazi codes**, invented a machine that scanned for patterns in messages intercepted from the Germans. The machine was called Colossus, and scanned 5.000 characters a second, reducing the workload from weeks to merely hours. Colossus was the first data processor. Two years later, in 1945, John Von Neumann published a paper on the Electronic Discrete Variable Automatic Computer (EDVAC), the first “documented” discussion on program storage, and laid the foundation of computer architecture today.\n",
    "\n",
    "### Creation of the NSA\n",
    "\n",
    "It is said these combined events prompted the “formal” creation of the **United States’ NSA (National Security Agency)**, by President Truman, in 1952. Staff at the NSA were assigned the task of decrypting messages intercepted during the Cold War. Computers of this time had evolved to the point where they could collect and process data, operating independently and automatically.\n",
    "\n",
    "\n",
    "### Creation of ARPANET\n",
    "\n",
    "**ARPANET** began on Oct 29, 1969, when a message was sent from UCLA’s host computer to Stanford’s host computer. It received funding from the Advanced Research Projects Agency (ARPA), a subdivision of the Department of Defense. Generally speaking, the public was not aware of ARPANET. In 1973, it connected with a transatlantic satellite, linking it to the Norwegian Seismic Array. However, by 1989, the infrastructure of ARPANET had started to age. The system wasn’t as efficient or as fast as newer networks. Organizations using ARPANET started moving to other networks, such as NSFNET, to improve basic efficiency and speed. In 1990, the ARPANET project was shut down, due to a combination of age and obsolescence. The creation ARPANET led directly to the Internet.\n",
    "\n",
    "### Taxes, Fingerprints, and Conspiracy Theories\n",
    "\n",
    "In 1965, the U.S. government built the first data center, with the intention of storing millions of **fingerprint sets and tax returns**. Each record was transferred to magnetic tapes, and were to be taken and stored in a central location. [Conspiracy theorists](https://news.google.com/newspapers?id=ZGogAAAAIBAJ&sjid=3GYFAAAAIBAJ&pg=933,5465131&dq=data-center&hl=en) expressed their fears, and the project was closed. However, in spite of its closure, this initiative is generally considered the first effort at large scale data storage.\n",
    "\n",
    "### World Wide Web\n",
    "\n",
    "In 1989, a British Computer Scientist named Tim Berners-Lee came up with the concept of the **World Wide Web**. The Web is a place/information-space where web resources are recognized using URLs, interlinked by hypertext links, and is accessible via the Internet. His system also allowed for the transfer of audio, video, and pictures. His goal was to share information on the Internet using a hypertext system. By the fall of 1990, Tim Berners-Lee, working for CERN, had written three basic IT commands that are the foundation of today’s web:\n",
    "\n",
    "- HTML: HyperText Markup Language. The formatting language of the web.\n",
    "- URL: Uniform Resource Locator. A unique “address” used to identify each resource on the web. It is also called a - URI (Uniform Resource Identifier).\n",
    "- HTTP: Hypertext Transfer Protocol. Used for retrieving linked resources from all across the web.\n",
    "\n",
    "In 1993, CERN announced the World Wide Web would be free for everyone to develop and use. The free part was a key factor in the effect the Web would have on the people of the world. **(It’s the companies providing the “internet connection” that charge us a fee)**.\n",
    "\n",
    "### Internet Of Things\n",
    "\n",
    "The concept of Internet of Things was assigned its official name in 1999. By 2013, the IoT had evolved to include multiple technologies, using the Internet, wireless communications, micro-electromechanical systems (MEMS), and embedded systems. All of these transmit data about the person using them. Automation (including buildings and homes), GPS, and others, support the IoT.\n",
    "\n",
    "The Internet of Things, unfortunately, can make computer systems vulnerable to hacking. In October of 2016, hackers crippled major portions of the Internet using the IoT. The early response has been to develop Machine Learning and Artificial Intelligence focused on security issues.\n",
    "\n",
    "### Big Data Gets A Name\n",
    "\n",
    "In 2005, Big Data, which had been used without a name, was labeled by Roger Mougalas. He was referring to a large set of data that, at the time, was almost impossible to manage and process using the traditional business intelligence tools available. Additionally, Hadoop, which could handle Big Data, was created in 2005. Hadoop was based on an open-sourced software framework called Nutch, and was merged with Google’s MapReduce. Hadoop is an Open Source software framework, and can process structured and unstructured data, from almost all digital sources. Because of this flexibility, Hadoop (and its sibling frameworks) can process Big Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## A Big Data Timetable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancient History of Data\n",
    "\n",
    "#### C 18,000 BCE\n",
    "\n",
    "The earliest examples we have of humans storing and analyzing data are the tally sticks. The Ishango Bone was discovered in 1960 in what is now Uganda and is thought to be one of the earliest pieces of evidence of prehistoric data storage. Palaeolithic tribespeople would mark notches into sticks or bones, to keep track of trading activity or supplies. They would compare sticks and notches to carry out rudimentary calculations, enabling them to make predictions such as how long their food supplies would last.\n",
    "\n",
    "#### C 2400 BCE\n",
    "\n",
    "The abacus – the first dedicated device constructed specifically for performing calculations – comes into use in Babylon. The first libraries also appeared around this time, representing our first attempts at mass data storage.\n",
    "\n",
    "#### 300 BC – 48 AD\n",
    "\n",
    "The Library of Alexandria is perhaps the largest collection of data in the ancient world, housing up to perhaps half a million scrolls and covering everything we had learned so far, about pretty much everything. Unfortunately, in 48AD it is thought to have been destroyed by the invading Romans, perhaps accidentally. Contrary to common myth, not everything was lost – significant parts of the library’s collections were moved to other buildings in the city, or stolen and dispersed throughout the ancient world.\n",
    "\n",
    "#### C 100 – 200 AD\n",
    "\n",
    "The Antikythera Mechanism, the earliest discovered mechanical computer, is produced, presumably by Greek scientists. It’s “CPU” consists of 30 interlocking bronze gears and it is thought to have been designed for astrological purposes and tracking the cycle of Olympic Games. Its design suggests it is probably an evolution of an earlier device – but these so far remain undiscovered.\n",
    "\n",
    "### The Emergence of Statistics\n",
    "\n",
    "#### 1663\n",
    "\n",
    "In London, John Graunt carries out the first recorded experiment in statistical data analysis. By recording information about mortality, he theorized that he can design an early warning system for the bubonic plague ravaging Europe.\n",
    "\n",
    "#### 1865\n",
    "\n",
    "The term “business intelligence” is used by Richard Millar Devens in his Encyclopaedia of Commercial and Business Anecdotes, describing how the banker Henry Furnese achieved an advantage over competitors by collecting and analyzing information relevant to his business activities in a structured manner. This is thought to be the first study of a business putting data analysis to use for commercial purposes.\n",
    "\n",
    "#### 1880\n",
    "\n",
    "The US Census Bureau has a problem – it estimates that it will take it 8 years to crunch all the data collected in the 1880 census, and it is predicted that the data generated by the 1890 census will take over 10 years, meaning it will not even be ready to look at until it is outdated by the 1900 census. In 1881 a young engineer employed by the bureau – Herman Hollerith – produces what will become known as the Hollerith Tabulating Machine. Using punch cards, he reduces 10 years’ work to three months and achieves his place in history as the father of modern automated computation. The company he founds will go on to become known as IBM.\n",
    "\n",
    "### The Early Days of Modern Data Storage\n",
    "\n",
    "#### 1926\n",
    "\n",
    "Interviewed by Colliers magazine, inventor Nikola Tesla states that when wireless technology is “perfectly applied the whole Earth will be converted into a huge brain, which in fact it is, all things being particles of a real and rhythmic whole … and the instruments through which we shall be able to do this will be amazingly simple compared to our present telephone. A man will be able to carry one in his vest pocket.”\n",
    "\n",
    "#### 1928\n",
    "\n",
    "Fritz Pfleumer, a German-Austrian engineer, invents a method of storing information magnetically on tape. The principles he develops are still in use today, with the vast majority of digital data being stored magnetically on computer hard disks.\n",
    "\n",
    "#### 1944\n",
    "\n",
    "Fremont Rider, librarian at Wesleyan University, Connecticut, US, published a paper titled The Scholar and the Future of the Research Library.\n",
    "\n",
    "In one of the earliest attempts to quantify the amount of information being produced, he observes that in order to store all the academic and popular works of value being produced, American libraries would have to double their capacity every 16 years. This led him to speculate that the Yale Library, by 2040, will contain 200 million books spread over 6,000 miles of shelves.\n",
    "\n",
    "### The Beginnings of Business Intelligence\n",
    "\n",
    "#### 1958\n",
    "\n",
    "IBM researcher Hans Peter Luhn defines Business Intelligence as “the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.”\n",
    "\n",
    "#### 1962\n",
    "\n",
    "The first steps are taken towards speech recognition, when IBM engineer William C Dersch presents the Shoebox Machine at the 1962 World Fair. It can interpret numbers and sixteen words spoken in the English language into digital information.\n",
    "\n",
    "#### 1964\n",
    "\n",
    "An article in the New Statesman refers to the difficulty in managing the increasing amount of information becoming available.\n",
    "\n",
    "### The Start of Large Data Centers\n",
    "\n",
    "#### 1965\n",
    "\n",
    "The US Government plans the world’s first data center to store 742 million tax returns and 175 million sets of fingerprints on magnetic tape.\n",
    "\n",
    "#### 1970\n",
    "\n",
    "IBM mathematician Edgar F Codd presents his framework for a “relational database”. The model provides the framework that many modern data services use today, to store information in a hierarchical format, which can be accessed by anyone who knows what they are looking for. Prior to this accessing data from a computer’s memory banks usually required an expert.\n",
    "\n",
    "#### 1976\n",
    "\n",
    "Material Requirements Planning (MRP) systems are becoming more commonly used across the business world, representing one of the first mainstream commercial uses of computers to speed up everyday processes and make efficiencies. Until now, most people have probably only seen them in research and development or academic settings.\n",
    "\n",
    "#### 1989\n",
    "\n",
    "Possibly the first use of the term Big Data (without capitalization) in the way it is used today. International best-selling author Erik Larson pens an article for Harpers Magazine speculating on the origin of the junk mail he receives. He writes: “The keepers of big data say they are doing it for the consumer’s benefit. But data have a way of being used for purposes other originally intended.”\n",
    "\n",
    "Additionally “business intelligence” – already a popular concept since the late 50s – sees a surge in popularity with newly emerging software and systems for analyzing commercial and operational performance.\n",
    "\n",
    "### The Emergence of the Internet\n",
    "\n",
    "#### 1991\n",
    "\n",
    "Computer scientist Tim Berners-Lee announced the birth of what would become the Internet as we know it today. In a post in the Usenet group alt.hypertext he sets out the specifications for a worldwide, interconnected web of data, accessible to anyone from anywhere.\n",
    "\n",
    "#### 1996\n",
    "\n",
    "According to R J T Morris and B J Truskowski in their 2003 book The Evolution of Storage Systems, this is the point where digital storage became more cost effective than paper.\n",
    "\n",
    "#### 1997\n",
    "\n",
    "Michael Lesk publishes his paper How Much Information is there in the World?Theorizing that the existence of 12,000 petabytes is “perhaps not an unreasonable guess”. He also points out that even at this early point in its development, the web is increasing in size 10-fold each year. Much of this data, he points out, will never be seen by anyone and therefore yield no insight.\n",
    "\n",
    "Google Search also debuts this year – and for the next 20 years (at least) its name will become shorthand for searching the internet for data.\n",
    "\n",
    "### Early Ideas of Big Data\n",
    "\n",
    "#### 1999\n",
    "\n",
    "A couple of years later and the term Big Data appears in Visually Exploring Gigabyte Datasets in Real Time, published by the Association for Computing Machinery. Again the propensity for storing large amounts of data with no way of adequately analyzing it is lamented. The paper goes on to quote computing pioneer Richard W Hamming as saying: “The purpose of computing is insight, not numbers.”\n",
    "\n",
    "Also possibly first use of the term “Internet of Things”, to describe the growing number of devices online and the potential for them to communicate with each other, often without a human “middle man”. The term is used as the title of a presentation given to Procter and Gamble by RFID pioneer Kevin Ashton.\n",
    "\n",
    "#### 2000\n",
    "\n",
    "In How Much Information? Peter Lyman and Hal Varian (now chief economist at Google) attempted to quantify the amount of digital information in the world, and its rate of growth, for the first time. They concluded: “The world’s total yearly production of print, film, optical and magnetic content would require roughly 1.5 billion gigabytes of storage. This is the equivalent of 250 megabytes per person for each man, woman and child on Earth.”\n",
    "\n",
    "#### 2001\n",
    "\n",
    "In his paper 3D Data Management: Controlling Data Volume, Velocity and VarietyDoug Laney, analyst at Gartner, defines three of what will come to be the commonly-accepted characteristics of Big Data.\n",
    "\n",
    "This year also see the first use of the term “software as a service” – a concept fundamental to many of the cloud-based applications which are industry-standard today – in the article Strategic Backgrounder: Software as a Service by the Software and Information Industry Association.\n",
    "\n",
    "### Web 2.0 Increases Data Volumes\n",
    "\n",
    "##### 2005\n",
    "\n",
    "Commentators announce that we are witnessing the birth of “Web 2.0” – the user-generated web where the majority of content will be provided by users of services, rather than the service providers themselves. This is achieved through integration of traditional HTML-style web pages with vast back-end databases built on SQL. 5.5 million people are already using Facebook, launched a year earlier, to upload and share their own data with friends.\n",
    "\n",
    "This year also sees the creation of Hadoop – the open source framework created specifically for storage and analysis of Big Data sets. Its flexibility makes it particularly useful for managing the unstructured data (voice, video, raw text etc) which we are increasingly generating and collecting.\n",
    "\n",
    "Today’s Use of the Term ‘Big Data’ Emerges\n",
    "\n",
    "#### 2007\n",
    "\n",
    "Wired brings the concept of Big Data to the masses with their article The End of Theory: The Data Deluge Makes the Scientific Model Obsolete.\n",
    "\n",
    "#### 2008\n",
    "\n",
    "The world’s servers process 9.57 zettabytes (9.57 trillion gigabytes) of information – equivalent to 12 gigabytes of information per person, per day), according to the How Much Information? 2010 report. In International Production and Dissemination of Information, it is estimated that 14.7 exabytes of new information are produced this year.\n",
    "\n",
    "#### 2009\n",
    "\n",
    "The average US company with over 1,000 employees is storing more than 200 terabytes of data according to the report Big Data: The Next Frontier for Innovation, Competition and Productivity by McKinsey Global Institute.\n",
    "\n",
    "#### 2010\n",
    "\n",
    "Eric Schmidt, executive chairman of Google, tells a conference that as much data is now being created every two days, as was created from the beginning of human civilization to the year 2003.\n",
    "\n",
    "#### 2011\n",
    "\n",
    "The McKinsey report states that by 2018 the US will face a shortfall of between 140,000 and 190,000 professional data scientists, and states that issues including privacy, security and intellectual property will have to be resolved before the full value of Big Data will be realised.\n",
    "\n",
    "#### 2014\n",
    "\n",
    "The rise of the mobile machines – as for the first time, more people are using mobile devices to access digital data, than office or home computers. 88% of business executives surveyed by GE working with Accenture report that big data analytics is a top priority for their business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Processing (OLTP, OLAP, RTAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Customer View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single customer view, also known as propensity modeling is an aggregated, consistent and holistic representation of the data held by an organisation about its customers that can be viewed in one place, such as a single page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/single_customer_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Article: [When Single Customer View Goes Wrong](https://icrunchdata.com/blog/138/when-single-customer-view-goes-wrong/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Single Customer View was a way to consolidate data into a single customer model that brought in all sources of information, cleansed of spelling mistakes and last known addresses to be a clean, pristine indication of the customer that was either standing in front of you or who was logged into your portal.\n",
    "\n",
    "> Not only did the solution claim to tell you everything that was true about that mythical unicorn that was labeled \"the perfect customer\" – it was also promised to tell you all aspects of them, a 360° view – including family circles and social networks, possibly employment and where they live and play.\n",
    "\n",
    "> The official definition is an aggregated, consistent and holistic representation of the data known by an organization about its customers. This is supposed to give companies the ability to analyze past behavior in order to better target and personalize future customer interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what is included in a **Single Customer View** implementation: \n",
    "\n",
    "- ### Locked highly marketed and very expensive brand name ecosystem data warehouse – check.\n",
    "- ### Million dollar data model that will only be filled by less 15% – check.\n",
    "- ### Convoluted Extract Transformation Load process muddled by quality and cleansing software to make load times run for 48 hours or more – check.\n",
    "- ### Business Intelligence reporting to show 300 reports asked by management that only 10 or less get looked at per quarter – and these are pivot table based – so they could have just used spreadsheets – check.\n",
    "- ### Fixed cost consulting services to implement proposed solution but professional services already bleeding money – check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "Copyright © 2020 Qualex Consulting Services Incorporated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
